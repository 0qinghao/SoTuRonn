
@article{å¾…ç¼–ç ç³»æ•° æ‰«æ äºŒå€¼åŒ– å“¥ä¼¦å¸ƒ è±æ–¯,
  title = {Transform {{Coefficient Coding}} in {{HEVC}}},
  author = {Sole, J. and Joshi, R. and Nguyen, N. and Ji, T. and Karczewicz, M. and Clare, G. and Henry, F. and Duenas, A.},
  date = {2012-12},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {22},
  pages = {1765--1777},
  issn = {1558-2205},
  doi = {10.1109/TCSVT.2012.2223055},
  abstract = {This paper describes transform coefficient coding in the draft international standard of High Efficiency Video Coding (HEVC) specification and the driving motivations behind its design. Transform coefficient coding in HEVC encompasses the scanning patterns and coding methods for the last significant coefficient, significance map, coefficient levels, and sign data. Special attention is paid to the new methods of last significant coefficient coding, multilevel significance maps, high-throughput binarization, and sign data hiding. Experimental results are provided to evaluate the performance of transform coefficient coding in HEVC.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems}} for {{Video Technology}}},
  file = {E\:\\Documents\\Zotero\\storage\\FGGUIEBA\\2012 - Transform Coefficient Coding in HEVC - Sole ç­‰ã€‚.pdf;E\:\\Documents\\Zotero\\storage\\REQEIKL2\\6324418.html},
  keywords = {coefficient levels,data encapsulation,draft international standard,Encoding,HEVC specification,high efficiency video coding,High Efficiency Video Coding (HEVC),high throughput entropy coder,high-throughput binarization,last significant coefficient coding,multilevel significance maps,scanning patterns,sign data hiding,Throughput,transform coding,transform coefficient coding,Transforms,video coding,Video coding},
  number = {12}
}

@article{é™æ€ å¸§å†… å„ç§ç®—æ³•å‹ç¼©ç‡ç»Ÿè®¡,
  title = {{{OVERVIEW AND BENCHMARKING SUMMARY FOR THE ICIP}} 2016 {{COMPRESSION CHALLENGE}}},
  author = {Alexiou, Evangelos and Viola, Irene and Krasula, Lukas and Richter, Thomas and Bruylants, Tim and Pinheiro, Antonio and Fliegel, Karel and Rerabek, Martin and Skodras, Athanassios and Schelkens, Peter and Ebrahimi, Touradj},
  date = {2016},
  pages = {38},
  file = {E\:\\Documents\\Zotero\\storage\\RBELLSP6\\Alexiou ç­‰ã€‚ - 2016 - OVERVIEW AND BENCHMARKING SUMMARY FOR THE ICIP 201.pdf},
  langid = {english}
}

@article{è¡Œç¼–ç  èŠ‚çœå­˜å‚¨ç©ºé—´,
  title = {An Energy-Efficient Low-Memory Image Compression System for Multimedia {{IoT}} Products},
  author = {Lee, Seong-Won and Kim, Ho-Young},
  date = {2018-12},
  journaltitle = {EURASIP Journal on Image and Video Processing},
  shortjournal = {J Image Video Proc.},
  volume = {2018},
  pages = {87},
  issn = {1687-5281},
  doi = {10.1186/s13640-018-0333-3},
  url = {https://jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0333-3},
  urldate = {2020-02-22},
  abstract = {Emerging Internet of things (IoT) technologies have rapidly expanded to multimedia applications, including highresolution image transmission. However, handling image data in IoT products with limited battery capacity requires low-complexity and small-size solutions such as low-memory compression techniques. The objective of this paper is to propose a line-based compression system based on four-level two-line discrete wavelet transform and adaptive line prediction. Bit stream is generated by multiplexing various frequency components with run-level coding combined with Huffman coding. The proposed system also includes a new bit rate control algorithm that could significantly improve image quality consistency in one frame. The proposed low-memory compression system can retain image quality for visually lossless compression criteria over the whole image frame. It can simultaneously lower total system power consumption in multimedia IoT products better than other existing low-memory compression techniques.},
  file = {E\:\\Documents\\Zotero\\storage\\N4GL59SF\\Lee å’Œ Kim - 2018 - An energy-efficient low-memory image compression s.pdf},
  langid = {english},
  number = {1}
}

@thesis{è¡Œç¼–ç  ä¸­ç§‘å¤§åšå£«è®ºæ–‡,
  title = {åŸºäºè¡Œç»“æ„çš„å›¾åƒç¼–ç },
  author = {å½­, ç§€è²},
  date = {2012},
  institution = {{ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦}},
  url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFD1214&filename=1012503624.nh&uid=WEEvREcwSlJHSldRa1FhcEE0QVN2K0s4bm54TUN3cTJGYWptMHZNTVZKTT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!&v=MzEzMjRITGE0SGRmT3E1RWJQSVI4ZVgxTHV4WVM3RGgxVDNxVHJXTTFGckNVUkxPZVplZHNGeWpsVXJyTFZGMjY=},
  urldate = {2019-12-13},
  abstract = {éšç€æ•°å­—å›¾åƒåœ¨å¤šåª’ä½“ä¸­çš„å¹¿æ³›åº”ç”¨ã€å›¾åƒåˆ†è¾¨ç‡çš„ä¸æ–­å¢åŠ ä»¥åŠæ–°çš„å›¾åƒè¡¨ç¤ºå½¢å¼çš„è¯ç”Ÿ,å›¾åƒç¼–ç æ— è®ºåœ¨ç¼–ç æ€§èƒ½è¿˜æ˜¯è®¡ç®—ã€å­˜å‚¨å¤æ‚åº¦ä¸Šéƒ½é¢ä¸´ç€æ–°çš„æŒ‘æˆ˜ã€‚ç°ä»Šä¸»æµçš„å›¾åƒå’Œå¸§å†…ç¼–ç æŠ€æœ¯éƒ½é‡‡ç”¨äº†åŸºäºå—ç»“æ„çš„ç¼–ç ,å…¶ä¸­æœ€å…ˆè¿›çš„ç¼–ç æ ‡å‡†H.264/AVCåœ¨å¸§å†…ç¼–ç æ—¶é‡‡ç”¨äº†å—ç»“æ„é¢„æµ‹å’ŒäºŒç»´å˜æ¢çš„æ–¹æ³•ã€‚ç„¶è€Œ,å—å—ç»“æ„çš„é™åˆ¶,å…¶é¢„æµ‹çš„æ€§èƒ½å¹¶ä¸å¥½,ä»è€Œå½±å“äº†å—ç»“æ„ç¼–ç çš„æ€§èƒ½ã€‚å¦ä¸€æ–¹é¢,å—ç»“æ„çš„ç¼–ç ä¸ºäº†æ¶ˆé™¤å—é—´çš„ç›¸å…³æ€§å¼•å…¥äº†è¾ƒå¼ºçš„å—é—´ç¼–ç ä¾èµ–æ€§,ä½¿å¾—å®ƒä¸é€‚äºé«˜å¹¶è¡Œåº¦çš„ç¼–ç æ¥æé«˜ç¼–ç é€Ÿåº¦,è€Œå—ç»“æ„çš„ç¼–ç ä»å­˜å‚¨å¤æ‚åº¦ä¸Šæ¥è¯´ä¹Ÿä¸æ˜¯æœ€ä¼˜çš„é€‰æ‹©ã€‚é’ˆå¯¹è¿™äº›é—®é¢˜,æœ¬æ–‡ä»æ”¹å˜ç¼–ç ç»“æ„çš„è§’åº¦å…¥æ‰‹,æå‡ºäº†åŸºäºè¡Œç»“æ„çš„ç¼–ç ã€‚ 	æœ¬æ–‡é¦–å…ˆ...},
  file = {E\:\\Documents\\Zotero\\storage\\Z46VVE7U\\å½­ - 2012 - åŸºäºè¡Œç»“æ„çš„å›¾åƒç¼–ç .caj},
  keywords = {block-based coding,broadcast,distributed source coding,H.264/AVC,image coding,parallel coding,prediction,transform,åˆ†å¸ƒå¼ç¼–ç ,å˜æ¢H.264/AVCå—ç»“æ„çš„ç¼–ç ,å›¾åƒç¼–ç ,å¹¶è¡Œç¼–ç ,å¹¿æ’­ä¼ è¾“,é¢„æµ‹},
  langid = {ä¸­æ–‡;},
  type = {åšå£«}
}

@inproceedings{é—ä¼ ç®—æ³• æ•°å€¼ç¼–ç å‹ç¼©,
  title = {Image Compression Based on Genetic Algorithm Optimization},
  booktitle = {2015 2nd {{World Symposium}} on {{Web Applications}} and {{Networking}} ({{WSWAN}})},
  author = {Omari, Mohammed and Yaichi, Salah},
  date = {2015-03},
  pages = {1--5},
  doi = {10.1109/WSWAN.2015.7210304},
  abstract = {Image compression has attracted a lot of research since the beginning of Internet era and telecommunication. Enhancing image compression quality and ratio was achieved through several approaches such as neural networks and discrete transforms. However, other heuristic and bio-inspired methods such as genetic algorithms are still under experimentation. In this paper, we introduced a new image compression mechanism based on exploiting the relationship between fractional numbers and their corresponding quotient representation. Each sub-image is mapped to a fractional number based on the RGB representation, and then reduced to an efficient quotient. The appeal of using genetic algorithms is explained by the massive search to find a close fraction that is reduced to short quotient. Our method showed a considerable compression ratio when the least significant bits of each byte are altered, hence, the image quality is preserved while achieving high compression ratio.},
  eventtitle = {2015 2nd {{World Symposium}} on {{Web Applications}} and {{Networking}} ({{WSWAN}})},
  file = {E\:\\Documents\\Zotero\\storage\\U96UTKKM\\Omari å’Œ Yaichi - 2015 - Image compression based on genetic algorithm optim.pdf;E\:\\Documents\\Zotero\\storage\\FLZGVK5X\\7210304.html},
  keywords = {Biological cells,data compression,Fractal image,fractional numbers,genetic algorithm optimization,genetic algorithms,Genetic algorithms,image coding,Image coding,image colour analysis,image compression quality enhancement,image compression ratio enhancement,image representation,Internet era,least significant bits,lossy compression,number theory,quotient representation,rational numbers,RGB representation,Sociology,Statistics,telecommunication,Wavelet transforms}
}

@article{DCT ç³»æ•° æ•°å­¦åˆ†æ,
  title = {A Mathematical Analysis of the {{DCT}} Coefficient Distributions for Images},
  author = {Lam, E. Y. and Goodman, J. W.},
  date = {2000-10},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {9},
  pages = {1661--1666},
  issn = {1941-0042},
  doi = {10.1109/83.869177},
  abstract = {Over the past two decades, there have been various studies on the distributions of the DCT coefficients for images. However, they have concentrated only on fitting the empirical data from some standard pictures with a variety of well-known statistical distributions, and then comparing their goodness of fit. The Laplacian distribution is the dominant choice balancing simplicity of the model and fidelity to the empirical data. Yet, to the best of our knowledge, there has been no mathematical justification as to what gives rise to this distribution. We offer a rigorous mathematical analysis using a doubly stochastic model of the images, which not only provides the theoretical explanations necessary, but also leads to insights about various other observations from the literature. This model also allows us to investigate how certain changes in the image statistics could affect the DCT coefficient distributions.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  file = {E\:\\Documents\\Zotero\\storage\\7JSMWCYC\\2000 - A mathematical analysis of the DCT coefficient distributions for images - Lam å’Œ Goodman.pdf},
  keywords = {central limit theorem,DCT coefficient distributions,discrete cosine transforms,Discrete cosine transforms,doubly stochastic model,empirical data,Gaussian distribution,Histograms,image coding,Image coding,image statistics,Laplace equations,Laplacian distribution,mathematical analysis,Mathematical analysis,Mathematical model,standard pictures,statistical distributions,Statistical distributions,stochastic processes,Stochastic processes,Testing,transform coding},
  number = {10}
}

@online{JPEG ITU æ ‡å‡†,
  title = {T.81~:~{{Information}} Technology - {{Digital}} Compression and Coding of Continuous-Tone Still Images - {{Requirements}} and Guidelines},
  url = {https://www.itu.int/rec/T-REC-T.81-199209-I/en},
  urldate = {2021-03-24}
}

@software{latex å…¥é—¨ æ•™å­¦ github repo,
  title = {Luong-Komorebi/{{Begin}}-{{Latex}}-in-Minutes},
  author = {Vo, Luong},
  date = {2021-03-22T16:17:15Z},
  origdate = {2016-10-30T11:01:23Z},
  url = {https://github.com/luong-komorebi/Begin-Latex-in-minutes},
  urldate = {2021-03-24},
  abstract = {ğŸ“œ Brief Intro to LaTeX for beginners that helps you use LaTeX with ease.},
  keywords = {basic,beginners,fast,guide,latex,latex-editor,latex-in-minutes,simple}
}

@online{VVC å¤æ‚åº¦åˆ†æ ç¼–ç  30x è§£ç  3x,
  title = {Complexity {{Analysis Of Next}}-{{Generation VVC Encoding And Decoding}}},
  url = {https://ieeexplore.ieee.org/document/9190983},
  urldate = {2021-03-04},
  abstract = {While the next generation video compression standard, Versatile Video Coding (VVC), provides a superior compression efficiency, its computational complexity dramatically increases. This paper thoroughly analyzes this complexity for both encoder and decoder of VVC Test Model 6, by quantifying the complexity break-down for each coding tool and measuring the complexity and memory requirements for VVC encoding/decoding. These extensive analyses are performed for six video sequences of 720p, 1080p, and 2160p, under Low-Delay (LD), Random-Access (RA), and All-Intra (AI) conditions (a total of 320 encoding/decoding). Results indicate that the VVC encoder and decoder are 5Ã— and 1.5Ã— more complex compared to HEVC in LD, and 31Ã— and 1.8Ã— in AI, respectively. Detailed analysis of coding tools reveals that in LD on average, motion estimation tools with 53\%, transformation and quantization with 22\%, and entropy coding with 7\% dominate the encoding complexity. In decoding, loop filters with 30\%, motion compensation with 20\%, and entropy decoding with 16\%, are the most complex modules. Moreover, the required memory bandwidth for VVC encoding/decoding are measured through memory profiling, which are 30Ã— and 3Ã— of HEVC. The reported results and insights are a guide for future research and implementations of energy-efficient VVC encoder/decoder.},
  file = {E\:\\Documents\\Zotero\\storage\\QFLIL2YL\\Complexity Analysis Of Next-Generation VVC Encoding And Decoding - .pdf},
  langid = {american}
}

@online{VVC å‹ç¼©ç‡ä¼˜åŒ– 30 percent,
  title = {Comparing {{VVC}}, {{HEVC}} and {{AV1}} Using {{Objective}} and {{Subjective Assessments}}},
  author = {Zhang, Fan and Katsenou, Angeliki V. and Afonso, Mariana and Dimitrov, Goce and Bull, David R.},
  date = {2020-03-23},
  url = {http://arxiv.org/abs/2003.10282},
  urldate = {2021-03-03},
  abstract = {In this paper, the performance of three state-ofthe-art video codecs: High Efficiency Video Coding (HEVC) Test Model (HM), AOMedia Video 1 (AV1) and Versatile Video Coding Test Model (VTM), are evaluated using both objective and subjective quality assessments. Nine source sequences were carefully selected to offer both diversity and representativeness, and different resolution versions were encoded by all three codecs at pre-defined target bitrates. The compression efficiency of the three codecs are evaluated using two commonly used objective quality metrics, PSNR and VMAF. The subjective quality of their reconstructed content is also evaluated through psychophysical experiments. Furthermore, HEVC and AV1 are compared within a dynamic optimization framework (convex hull rate-distortion optimization) across resolutions with a wider bitrate, using both objective and subjective evaluations. Finally the computational complexities of three tested codecs are compared. The subjective assessments indicate that, for the tested versions there is no significant difference between AV1 and HM, while the tested VTM version shows significant enhancements. The selected source sequences, compressed video content and associated subjective data are available online, offering a resource for compression performance evaluation and objective video quality assessment.},
  archiveprefix = {arXiv},
  eprint = {2003.10282},
  eprinttype = {arxiv},
  file = {E\:\\Documents\\Zotero\\storage\\XLPBV5A5\\2020 - Comparing VVC, HEVC and AV1 using Objective and Subjective Assessments - Zhang ç­‰ã€‚.pdf},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing},
  langid = {english},
  primaryclass = {eess}
}

@inproceedings{VVC å‹ç¼©ç‡ä¼˜åŒ– 40 percent æ—¶é—´ 15x,
  title = {Rate-{{Distortion}} and {{Complexity Comparison}} of {{HEVC}} and {{VVC Video Encoders}}},
  booktitle = {2020 {{IEEE}} 11th {{Latin American Symposium}} on {{Circuits}} \& {{Systems}} ({{LASCAS}})},
  author = {Siqueira, Icaro and Correa, Guilherme and Grellert, Mateus},
  date = {2020-02},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{San Jose, Costa Rica}},
  doi = {10.1109/LASCAS45839.2020.9069036},
  url = {https://ieeexplore.ieee.org/document/9069036/},
  urldate = {2021-03-04},
  abstract = {Video-coding systems have presented significant improvements driven by the wide adoption of video streaming technologies combined with demands for better quality from users. The most recent video-coding standard from JCT-VC, named High Efficiency Video Coding, greatly improved the compression rate compared to its predecessor, H.264/AVC, but an even better performance must be pursued to accommodate future technologies. This article presents a comparison between the current state-of-art HEVC standard with the most recent project that is being conducted by the same group of experts, entitled Versatile Video Coding (VVC). According to experimental results obtained using similar configurations for both encoders, the VVC reference software provides significant bit-rate savings of 44.4\% on average when compared to HEVC. However, this compression gains come with high computational costs: the VVC encoding time is on average 10.2 times higher when SIMD are used, and 15.9 times higher without such optimizations.},
  eventtitle = {2020 {{IEEE}} 11th {{Latin American Symposium}} on {{Circuits}} \& {{Systems}} ({{LASCAS}})},
  file = {E\:\\Documents\\Zotero\\storage\\3FJ9AMHU\\2020 - Rate-Distortion and Complexity Comparison of HEVC and VVC Video Encoders - Siqueira ç­‰ã€‚.pdf},
  isbn = {978-1-72813-427-7},
  langid = {english}
}


